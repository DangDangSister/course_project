{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Convolutional Neural Netrowk Model - DEAP Arousal emotion (Tw Classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import load_model\n",
    "from keras import metrics\n",
    "from keras import regularizers\n",
    "import models.nnmodels as nn\n",
    "import utils.data as data\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data for two classes classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DEAP dataset...\n",
      "DEAP dataset is loaded.\n",
      "\n",
      "##### Dataset representation for each subject #####\n",
      "\n",
      "There are 32 subjects in the dataset.\n",
      "\n",
      "(Trail/Video, Channel, Readings) :  (40, 40, 8064)\n",
      "(Trail/Video, label (valence or arousal)) :  (40, 2)\n"
     ]
    }
   ],
   "source": [
    "print 'Loading DEAP dataset...'\n",
    "subject_data_folds, valence_folds, arousal_folds = data.load_deap(32, 2)\n",
    "print 'DEAP dataset is loaded.\\n'\n",
    "\n",
    "print 5*'#' + ' Dataset representation for each subject ' + 5*'#'\n",
    "print '\\nThere are {} subjects in the dataset.\\n'.format(len(subject_data_folds))\n",
    "print '(Trail/Video, Channel, Readings) : ', subject_data_folds[0].shape\n",
    "print '(Trail/Video, label (valence or arousal)) : ', arousal_folds[0].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce the features dimention (8064D >> 101D)\n",
    "## Then Normalize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping features into Gaussian with mean zero and unit variance...\n",
      "Normalization done.\n"
     ]
    }
   ],
   "source": [
    "# Reduce the features dimension\n",
    "print 'Mapping features into Gaussian with mean zero and unit variance...'\n",
    "reduced_data_folds = data.normalize_features(subject_data_folds, flag=True)\n",
    "print 'Normalization done.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN model for two classes (low Valence / High Valence) classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Two Classes Classification for Valence emotion Using Convolutional Neural Network #####\n",
      "\n",
      "(40, 4040)\n",
      "(1240, 4040)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer conv1d_2: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e9e0026f0d10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Valence classification model for two classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCnn1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, emotion='valence')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/omid/course_project/models/nnmodels.py\u001b[0m in \u001b[0;36mCnn1D\u001b[0;34m(num_classes)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     model.add(Conv1D(filters=16, kernel_size=5, batch_input_shape=(None, 4040),\\\n\u001b[0;32m--> 102\u001b[0;31m                         kernel_initializer='he_normal'))\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/omid/venv2/local/lib/python2.7/site-packages/Keras-2.1.2-py2.7.egg/keras/models.pyc\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    463\u001b[0m                 \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m                 \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/omid/venv2/local/lib/python2.7/site-packages/Keras-2.1.2-py2.7.egg/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/omid/venv2/local/lib/python2.7/site-packages/Keras-2.1.2-py2.7.egg/keras/engine/topology.pyc\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    458\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    461\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer conv1d_2: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "num_exp = 40\n",
    "num_classes = 2\n",
    "batch_size = 50\n",
    "epochs = 10\n",
    "num_subjects = 32\n",
    "\n",
    "\n",
    "accuracies= []\n",
    "\n",
    "print '##### Two Classes Classification for Valence emotion Using Convolutional Neural Network #####\\n'\n",
    "   \n",
    "\n",
    "for s in np.arange(0, num_subjects):\n",
    "    \n",
    "    \n",
    "    X_val = reduced_data_folds[s]\n",
    "    print X_val.shape\n",
    "    #X_val = X_val.reshape(X_val.shape[0], 1, 40, 101)\n",
    "    y_val = valence_folds[s]\n",
    "    \n",
    "    X_remaining_folds = reduced_data_folds[:s] + reduced_data_folds[s+1:]\n",
    "    y_remaining_folds = valence_folds[:s] + valence_folds[s+1:]\n",
    "    \n",
    "    X_train = np.concatenate(X_remaining_folds)\n",
    "    print X_train.shape\n",
    "    #X_train = X_train.reshape(X_train.shape[0], 1, 40, 101)\n",
    "    \n",
    "    y_train = np.concatenate(y_remaining_folds)\n",
    "    \n",
    "    \n",
    "    key = 's'+str(s+1).zfill(2)+'_valence(2classes)'\n",
    "    \n",
    "    \n",
    "    # Valence classification model for two classes    \n",
    "    model = nn.Cnn1D(num_classes=num_classes)#, emotion='valence')\n",
    "    \n",
    "    \n",
    "    # checkpoint\n",
    "    filepath=\"../saved_models/best_CNN_model_for_{}.h5\".format(key)\n",
    "    \n",
    "    print 'Training model to test on subject {} ...\\n'.format(str(s+1).zfill(2))\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy',\n",
    "                                verbose=1, save_best_only=True, mode='max', period=20)\n",
    "    \n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    \n",
    "    hist = model.fit(X_train, y_train, \n",
    "             batch_size=batch_size,\n",
    "             epochs=epochs,\n",
    "             shuffle=True,\n",
    "             verbose=1,\n",
    "             validation_data=(X_val, y_val), callbacks=callbacks_list)\n",
    "\n",
    "    \n",
    "    \n",
    "    model.load_weights(filepath)\n",
    "    \n",
    "    scores = model.evaluate(x=X_val, y=y_val)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print '#### Result of CNN model for subject {} #####\\n'.format(str(s+1).zfill(2))\n",
    "    print \"Loss: {} , Accuracy : {}%\".format(scores[0], scores[1]*100)\n",
    "    \n",
    "    \n",
    "\n",
    "    accuracies.append(scores[1])\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(hist.history['categorical_accuracy'])\n",
    "    plt.plot(hist.history['val_categorical_accuracy'])\n",
    "    plt.title('Subject ' +str(s+1).zfill(2)+ ' CNN model accuracy for Valence')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # summarize history for loss\n",
    "    plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.title('Subject ' +str(s+1).zfill(2)+ ' CNN model loss for Valence')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    del model\n",
    " \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Accuracies per subject #####\n",
      "Subject 16 has the minimum accuracy : 55.0%\n",
      "Subject 19 has the maximum accuracy : 87.5%\n",
      "Average accuracy : 73.515625%\n",
      "Accuracy of Subject 1 : 65.0%\n",
      "Accuracy of Subject 2 : 75.0%\n",
      "Accuracy of Subject 3 : 72.5%\n",
      "Accuracy of Subject 4 : 75.0%\n",
      "Accuracy of Subject 5 : 80.0%\n",
      "Accuracy of Subject 6 : 77.5%\n",
      "Accuracy of Subject 7 : 72.5%\n",
      "Accuracy of Subject 8 : 70.0%\n",
      "Accuracy of Subject 9 : 70.0%\n",
      "Accuracy of Subject 10 : 77.5%\n",
      "Accuracy of Subject 11 : 60.0%\n",
      "Accuracy of Subject 12 : 80.0%\n",
      "Accuracy of Subject 13 : 77.5%\n",
      "Accuracy of Subject 14 : 77.5%\n",
      "Accuracy of Subject 15 : 72.5%\n",
      "Accuracy of Subject 16 : 55.0%\n",
      "Accuracy of Subject 17 : 77.5%\n",
      "Accuracy of Subject 18 : 80.0%\n",
      "Accuracy of Subject 19 : 87.5%\n",
      "Accuracy of Subject 20 : 87.5%\n",
      "Accuracy of Subject 21 : 70.0%\n",
      "Accuracy of Subject 22 : 70.0%\n",
      "Accuracy of Subject 23 : 72.5%\n",
      "Accuracy of Subject 24 : 77.5%\n",
      "Accuracy of Subject 25 : 65.0%\n",
      "Accuracy of Subject 26 : 75.0%\n",
      "Accuracy of Subject 27 : 77.5%\n",
      "Accuracy of Subject 28 : 75.0%\n",
      "Accuracy of Subject 29 : 65.0%\n",
      "Accuracy of Subject 30 : 72.5%\n",
      "Accuracy of Subject 31 : 82.5%\n",
      "Accuracy of Subject 32 : 60.0%\n"
     ]
    }
   ],
   "source": [
    "min_acc_subject_idx, max_acc_subject_idx = np.argmin(accuracies), np.argmax(accuracies)\n",
    "\n",
    "avg_acc =  reduce(lambda x, y: x + y, accuracies) / len(accuracies)\n",
    "\n",
    "print 5*'#' + ' Accuracies per subject ' + 5*'#'\n",
    "\n",
    "print 'Subject {} has the minimum accuracy : {}%'.format(min_acc_subject_idx + 1, min(accuracies)*100)\n",
    "print 'Subject {} has the maximum accuracy : {}%'.format(max_acc_subject_idx + 1, max(accuracies)*100)\n",
    "print 'Average accuracy : {}%'.format(avg_acc*100)\n",
    "\n",
    "for subj in enumerate(accuracies):\n",
    "    print 'Accuracy of Subject {} : {}%'.format(subj[0]+1, subj[1]*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
